{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"autogen-agentchat\"\n",
    "%pip install \"autogen-ext[openai]\"\n",
    "%pip install \"autogen-ext[azure]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMS\n",
    "\n",
    "Con OpenAI\n",
    "\n",
    "```python\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "openai_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY environment variable set.\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Con Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m az_model_client \u001b[38;5;241m=\u001b[39m AzureOpenAIChatCompletionClient(\n\u001b[0;32m      7\u001b[0m     azure_deployment\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m az_model_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCuál es la capital de Francia?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AliciaSanJuliánFerna\\Documents\\Python\\autogen\\venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:454\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[1;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_output\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m json_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel does not support JSON output.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m oai_messages_nested \u001b[38;5;241m=\u001b[39m [\u001b[43mto_oai_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[0;32m    455\u001b[0m oai_messages \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m oai_messages_nested \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_calling\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tools) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\AliciaSanJuliánFerna\\Documents\\Python\\autogen\\venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:222\u001b[0m, in \u001b[0;36mto_oai_type\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [assistant_message_to_oai(message)]\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtool_message_to_oai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AliciaSanJuliánFerna\\Documents\\Python\\autogen\\venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:192\u001b[0m, in \u001b[0;36mtool_message_to_oai\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtool_message_to_oai\u001b[39m(\n\u001b[0;32m    189\u001b[0m     message: FunctionExecutionResultMessage,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[ChatCompletionToolMessageParam]:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 192\u001b[0m         ChatCompletionToolMessageParam(content\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mcontent, role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m, tool_call_id\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mcall_id) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    193\u001b[0m     ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from autogen_core.models import UserMessage\n",
    "import os\n",
    "\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment= \"gpt-4o-mini\", \n",
    "    model=\"gpt-4o-mini\",  \n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "result = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=8) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "\n",
    "openai_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "result = await openai_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mensajes \n",
    "\n",
    "En **AutoGen AgentChat**, los mensajes son la base de la comunicación entre agentes, orquestadores y aplicaciones. Se dividen en dos grandes categorías:\n",
    "\n",
    "1. **Mensajes entre agentes** (Agent-Agent Messages)\n",
    "2. **Eventos y mensajes internos de un agente** (Internal Events)\n",
    "\n",
    "### **1. Mensajes entre agentes (Agent-Agent Messages)**\n",
    "\n",
    "Estos mensajes permiten la comunicación entre agentes y pertenecen al tipo `ChatMessage`. Pueden ser:\n",
    "\n",
    "- **Mensajes de texto** (`TextMessage`)\n",
    "- **Mensajes multimodales** (`MultiModalMessage`), que pueden incluir imágenes.\n",
    "\n",
    "#### **Ejemplo: Creación de un mensaje de texto**\n",
    "\n",
    "Para enviar un mensaje de texto entre agentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='User' models_usage=None content='Hola, mundo!' type='TextMessage'\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "text_message = TextMessage(content=\"Hola, mundo!\", source=\"User\")\n",
    "print(text_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo: Creación de un mensaje multimodal**\n",
    "\n",
    "Para enviar un mensaje con una imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='User' models_usage=None content=['¿Puedes describir esta imagen?', <autogen_core._image.Image object at 0x00000159448792B0>] type='MultiModalMessage'\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "from autogen_agentchat.messages import MultiModalMessage\n",
    "from autogen_core import Image as AGImage\n",
    "from PIL import Image\n",
    "\n",
    "# Cargar una imagen desde una URL\n",
    "pil_image = Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\n",
    "img = AGImage(pil_image)\n",
    "\n",
    "# Crear un mensaje multimodal con texto e imagen\n",
    "multi_modal_message = MultiModalMessage(content=[\"¿Puedes describir esta imagen?\", img], source=\"User\")\n",
    "print(multi_modal_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Eventos internos del agente (Internal Events)**\n",
    "\n",
    "Además de los mensajes entre agentes, existen **eventos internos** que representan acciones dentro de un agente. Estos eventos pertenecen al tipo `AgentEvent`.\n",
    "\n",
    "Algunos ejemplos de eventos internos:\n",
    "\n",
    "- **`ToolCallRequestEvent`** → Indica que un agente ha solicitado usar una herramienta.\n",
    "- **`ToolCallExecutionEvent`** → Contiene los resultados de la ejecución de una herramienta.\n",
    "\n",
    "Los eventos internos se generan automáticamente y se almacenan en el campo `inner_messages` dentro de la respuesta del método `on_messages()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Agents**\n",
    "\n",
    "AutoGen proporciona un conjunto de agentes predefinidos que pueden responder a mensajes de diferentes maneras. Todos los agentes comparten ciertos atributos y métodos clave:\n",
    "\n",
    "## **Atributos y Métodos Comunes**\n",
    "\n",
    "- `name`: Nombre único del agente.\n",
    "- `description`: Descripción en texto del agente.\n",
    "- `on_messages()`: Recibe una secuencia de mensajes y devuelve una respuesta. **Modifica el estado del agente**, por lo que no se debe llamar con el historial completo de mensajes.\n",
    "- `on_messages_stream()`: Igual que `on_messages()`, pero devuelve un iterador de eventos `AgentEvent` o `ChatMessage`, seguido de una `Response` final.\n",
    "- `on_reset()`: Restablece el agente a su estado inicial.\n",
    "- `run()` y `run_stream()`: Métodos de conveniencia que llaman a `on_messages()` y `on_messages_stream()` respectivamente, pero con la misma interfaz que los equipos de agentes.\n",
    "\n",
    "## **Ejemplo: Creando un Assistant Agent**\n",
    "\n",
    "`AssistantAgent` es un agente predefinido que usa un modelo de lenguaje y puede ejecutar herramientas.\n",
    "\n",
    "### **Paso 1: Importar las librerías necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Definir una herramienta personalizada**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herramienta para buscar información en la web.\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Encuentra información en la web\"\"\"\n",
    "    return \"AutoGen es un framework de programación para construir aplicaciones multiagente.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Crear el agente con un modelo de OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=openai_model_client,\n",
    "    tools=[web_search],  # Asigna la herramienta de búsqueda\n",
    "    system_message=\"Usa herramientas para resolver tareas.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obteniendo Respuestas**\n",
    "\n",
    "El método `on_messages()` permite obtener respuestas a los mensajes enviados al agente.\n",
    "\n",
    "### **Ejemplo: Obtener una respuesta del asistente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=64, completion_tokens=16), content=[FunctionCall(id='call_rgqHzmkfi5saRWNFKscn1717', arguments='{\"query\":\"AutoGen\"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='AutoGen es un framework de programación para construir aplicaciones multiagente.', call_id='call_rgqHzmkfi5saRWNFKscn1717')], type='ToolCallExecutionEvent')]\n",
      "source='assistant' models_usage=None content='AutoGen es un framework de programación para construir aplicaciones multiagente.' type='ToolCallSummaryMessage'\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run() -> None:\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=\"Encuentra información sobre AutoGen\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    print(response.inner_messages)  # Mensajes internos del agente\n",
    "    print(response.chat_message)  # Respuesta final del agente\n",
    "\n",
    "# Para ejecutar en un script:\n",
    "# asyncio.run(assistant_run())\n",
    "await assistant_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Respuestas en Streaming**\n",
    "\n",
    "Podemos recibir respuestas en tiempo real con `on_messages_stream()`. También se puede utilizar `Console` para visualizar los mensajes en la consola.\n",
    "\n",
    "### **Ejemplo: Respuesta en streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- assistant ----------\n",
      "AutoGen es un framework de programación diseñado para construir aplicaciones multiagente. Los frameworks multiagentes permiten crear aplicaciones donde múltiples agentes inteligentes pueden interactuar y trabajar en conjunto para lograr objetivos complejos de manera autónoma y organizada. Sin embargo, podría haber más detalles y contextos específicos sobre AutoGen dependiendo del ámbito particular (tecnológico, industrial, etc.) en el que se esté utilizando o discutiendo. Si necesitas información más detallada, puedo realizar una búsqueda más profunda o específica.\n"
     ]
    }
   ],
   "source": [
    "async def assistant_run_stream() -> None:\n",
    "    await Console(\n",
    "        agent.on_messages_stream(\n",
    "            [TextMessage(content=\"Encuentra información sobre AutoGen\", source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )\n",
    "# Para ejecutar en un script:\n",
    "# asyncio.run(assistant_run_stream())\n",
    "await assistant_run_stream()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Otros Agentes Predefinidos**\n",
    "\n",
    "AutoGen también proporciona otros agentes con diferentes funcionalidades:\n",
    "\n",
    "- **UserProxyAgent**: Devuelve la entrada del usuario como respuesta.\n",
    "- **CodeExecutorAgent**: Puede ejecutar código.\n",
    "- **OpenAIAssistantAgent**: Conexión con un OpenAI Assistant y herramientas personalizadas.\n",
    "- **MultimodalWebSurfer**: Busca y visita páginas web.\n",
    "- **FileSurfer**: Explora archivos locales.\n",
    "- **VideoSurfer**: Analiza información de videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **Uso de Herramientas**\n",
    "\n",
    "Los modelos de lenguaje por sí solos están limitados a generar texto. Sin embargo, **AutoGen permite que los agentes usen herramientas externas**, como llamadas a APIs o bases de datos.\n",
    "\n",
    "📌 **Tool Calling / Function Calling**: Los modelos modernos pueden aceptar herramientas definidas por el usuario y generar un mensaje de llamada a una herramienta.\n",
    "\n",
    "### **Ejemplo: Agente con herramientas de LangChain**\n",
    "\n",
    "Podemos integrar herramientas de LangChain usando `LangChainToolAdapter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- assistant ----------\n",
      "[FunctionCall(id='call_2sKC2cshNTlFSpKuwpRwsTHD', arguments='{\"query\":\"df[\\'Age\\'].mean()\"}', name='python_repl_ast')]\n",
      "---------- assistant ----------\n",
      "[FunctionExecutionResult(content='29.69911764705882', call_id='call_2sKC2cshNTlFSpKuwpRwsTHD')]\n",
      "---------- assistant ----------\n",
      "29.69911764705882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(chat_message=ToolCallSummaryMessage(source='assistant', models_usage=None, content='29.69911764705882', type='ToolCallSummaryMessage'), inner_messages=[ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=114, completion_tokens=22), content=[FunctionCall(id='call_2sKC2cshNTlFSpKuwpRwsTHD', arguments='{\"query\":\"df[\\'Age\\'].mean()\"}', name='python_repl_ast')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='29.69911764705882', call_id='call_2sKC2cshNTlFSpKuwpRwsTHD')], type='ToolCallExecutionEvent')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "\n",
    "# Cargar un dataset de Titanic en Pandas\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\")\n",
    "\n",
    "# Crear herramienta de análisis con LangChain\n",
    "tool = LangChainToolAdapter(PythonAstREPLTool(locals={\"df\": df}))\n",
    "\n",
    "# Configurar el agente con herramientas de LangChain\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    tools=[tool],\n",
    "    model_client=openai_model_client,\n",
    "    system_message=\"Usa la variable `df` para acceder al dataset.\",\n",
    ")\n",
    "\n",
    "# Consultar la edad promedio de los pasajeros\n",
    "await Console(\n",
    "    agent.on_messages_stream(\n",
    "        [TextMessage(content=\"¿Cuál es la edad promedio de los pasajeros?\", source=\"user\")], \n",
    "        CancellationToken()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Llamadas a Herramientas en Paralelo**\n",
    "\n",
    "Algunos modelos permiten ejecutar múltiples herramientas a la vez.\n",
    "\n",
    "📌 Para desactivar esta opción en `OpenAIChatCompletionClient`, establece `parallel_tool_calls=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client_no_parallel_tool_call = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    parallel_tool_calls=False,  # Desactiva llamadas paralelas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Salida Estructurada con JSON**\n",
    "\n",
    "Podemos hacer que el modelo devuelva respuestas estructuradas en JSON usando `Pydantic`.\n",
    "\n",
    "### **Ejemplo: Respuesta con formato JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "I am happy.\n",
      "---------- assistant ----------\n",
      "{\"thoughts\":\"The user explicitly states their emotion using the word 'happy', which clearly indicates their emotional state.\",\"response\":\"happy\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='I am happy.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=88, completion_tokens=28), content='{\"thoughts\":\"The user explicitly states their emotion using the word \\'happy\\', which clearly indicates their emotional state.\",\"response\":\"happy\"}', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Modelo de respuesta estructurada\n",
    "class AgentResponse(BaseModel):\n",
    "    thoughts: str\n",
    "    response: Literal[\"happy\", \"sad\", \"neutral\"]\n",
    "\n",
    "# Crear agente con salida estructurada\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    response_format=AgentResponse,  # Especifica el formato de respuesta\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Categoriza el input como 'happy', 'sad' o 'neutral'.\",\n",
    ")\n",
    "\n",
    "await Console(agent.run_stream(task=\"I am happy.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
